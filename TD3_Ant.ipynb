{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"WXu1r8qvSzWf"},"source":["# Twin-Delayed DDPG"]},{"cell_type":"markdown","metadata":{"id":"YRzQUhuUTc0J"},"source":["## Installing the packages"]},{"cell_type":"code","metadata":{"id":"HAHMB0Ze8fU0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671226975209,"user_tz":-540,"elapsed":30080,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"346dc0ae-cef8-4437-c96a-194df32fb540"},"source":["!pip install pybullet\n","!pip install gym==0.22.0"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pybullet\n","  Downloading pybullet-3.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n","\u001b[K     |████████████████████████████████| 91.7 MB 163 kB/s \n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-3.2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym==0.22.0\n","  Downloading gym-0.22.0.tar.gz (631 kB)\n","\u001b[K     |████████████████████████████████| 631 kB 29.5 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.22.0) (1.5.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.22.0) (0.0.8)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.22.0) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.22.0) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym==0.22.0) (3.11.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.22.0-py3-none-any.whl size=708394 sha256=463a205d8bda5c873187fceb3646321f54ae89c3006c0ffbac420ad5271a83d0\n","  Stored in directory: /root/.cache/pip/wheels/4d/6e/7d/0e050cb65b427dd0db5c5ab2c9a1f1ffa3d58db2f4db516268\n","Successfully built gym\n","Installing collected packages: gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.22.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xjm2onHdT-Av"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"Ikr2p0Js8iB4","executionInfo":{"status":"ok","timestamp":1671226978184,"user_tz":-540,"elapsed":2979,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4504a95-7994-4a22-ed8e-a8d14bc014ee"},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pybullet_envs\n","import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gym import wrappers\n","from torch.autograd import Variable\n","from collections import deque"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y2nGdtlKVydr"},"source":["## Step 1: We initialize the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"u5rW0IDB8nTO","executionInfo":{"status":"ok","timestamp":1671226978184,"user_tz":-540,"elapsed":5,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["class ReplayBuffer(object):\n","\n","  def __init__(self, max_size=1e6):\n","    self.storage = []\n","    self.max_size = max_size\n","    self.ptr = 0\n","\n","  def add(self, transition):\n","    if len(self.storage) == self.max_size:\n","      self.storage[int(self.ptr)] = transition\n","      self.ptr = (self.ptr + 1) % self.max_size\n","    else:\n","      self.storage.append(transition)\n","\n","  def sample(self, batch_size):\n","    ind = np.random.randint(0, len(self.storage), size=batch_size)\n","    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n","    for i in ind: \n","      state, next_state, action, reward, done = self.storage[i]\n","      batch_states.append(np.array(state, copy=False))\n","      batch_next_states.append(np.array(next_state, copy=False))\n","      batch_actions.append(np.array(action, copy=False))\n","      batch_rewards.append(np.array(reward, copy=False))\n","      batch_dones.append(np.array(done, copy=False))\n","    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jb7TTaHxWbQD"},"source":["## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"]},{"cell_type":"code","metadata":{"id":"4CeRW4D79HL0","executionInfo":{"status":"ok","timestamp":1671226978184,"user_tz":-540,"elapsed":4,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["class Actor(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return x"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRDDce8FXef7"},"source":["## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"]},{"cell_type":"code","metadata":{"id":"OCee7gwR9Jrs","executionInfo":{"status":"ok","timestamp":1671226978184,"user_tz":-540,"elapsed":4,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["class Critic(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzIDuONodenW"},"source":["## Steps 4 to 15: Training Process"]},{"cell_type":"code","metadata":{"id":"zzd0H1xukdKe","executionInfo":{"status":"ok","timestamp":1671226978818,"user_tz":-540,"elapsed":638,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","    \n","    for it in range(iterations):\n","      \n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","      \n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","      \n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","      \n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","      \n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","      \n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","      \n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","      \n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","      \n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","      \n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        \n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","        \n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","  \n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","  \n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ka-ZRtQvjBex"},"source":["## We make a function that evaluates the policy by calculating its average reward over 10 episodes"]},{"cell_type":"code","metadata":{"id":"qabqiYdp9wDM","executionInfo":{"status":"ok","timestamp":1671226978818,"user_tz":-540,"elapsed":11,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGuKmH_ijf7U"},"source":["## We set the parameters"]},{"cell_type":"code","metadata":{"id":"HFj6wbAo97lk","executionInfo":{"status":"ok","timestamp":1671226978818,"user_tz":-540,"elapsed":11,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n","seed = 0 # Random seed number\n","start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n","eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n","max_timesteps = 5e5 # Total number of iterations/timesteps\n","save_models = True # Boolean checker whether or not to save the pre-trained model\n","expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n","batch_size = 100 # Size of the batch\n","discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n","tau = 0.005 # Target network update rate\n","policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n","noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n","policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hjwf2HCol3XP"},"source":["## We create a file name for the two saved models: the Actor and Critic models"]},{"cell_type":"code","metadata":{"id":"1fyH8N5z-o3o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671226978818,"user_tz":-540,"elapsed":10,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"81dfb3c8-d853-4dfc-b809-41d553f3e32f"},"source":["file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"kop-C96Aml8O"},"source":["## We create a folder inside which will be saved the trained models"]},{"cell_type":"code","metadata":{"id":"Src07lvY-zXb","executionInfo":{"status":"ok","timestamp":1671226978819,"user_tz":-540,"elapsed":5,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["if not os.path.exists(\"./results\"):\n","  os.makedirs(\"./results\")\n","if save_models and not os.path.exists(\"./pytorch_models\"):\n","  os.makedirs(\"./pytorch_models\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEAzOd47mv1Z"},"source":["## We create the PyBullet environment"]},{"cell_type":"code","metadata":{"id":"CyQXJUIs-6BV","executionInfo":{"status":"ok","timestamp":1671226978819,"user_tz":-540,"elapsed":5,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["env = gym.make(env_name)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YdPG4HXnNsh"},"source":["## We set seeds and we get the necessary information on the states and actions in the chosen environment"]},{"cell_type":"code","metadata":{"id":"Z3RufYec_ADj","executionInfo":{"status":"ok","timestamp":1671226978819,"user_tz":-540,"elapsed":4,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWEgDAQxnbem"},"source":["## We create the policy network (the Actor model)"]},{"cell_type":"code","metadata":{"id":"wTVvG7F8_EWg","executionInfo":{"status":"ok","timestamp":1671226983041,"user_tz":-540,"elapsed":4226,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["policy = TD3(state_dim, action_dim, max_action)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZI60VN2Unklh"},"source":["## We create the Experience Replay memory"]},{"cell_type":"code","metadata":{"id":"sd-ZsdXR_LgV","executionInfo":{"status":"ok","timestamp":1671226983041,"user_tz":-540,"elapsed":3,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["replay_buffer = ReplayBuffer()"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYOpCyiDnw7s"},"source":["## We define a list where all the evaluation results over 10 episodes are stored"]},{"cell_type":"code","metadata":{"id":"dhC_5XJ__Orp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671226986673,"user_tz":-540,"elapsed":3634,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"e43e1cab-89c8-455c-8e6e-b79795930d94"},"source":["evaluations = [evaluate_policy(policy)]"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Average Reward over the Evaluation Step: 9.807990\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"xm-4b3p6rglE"},"source":["## We create a new folder directory in which the final results (videos of the agent) will be populated"]},{"cell_type":"code","metadata":{"id":"MTL9uMd0ru03","executionInfo":{"status":"ok","timestamp":1671226986673,"user_tz":-540,"elapsed":3,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["def mkdir(base, name):\n","    path = os.path.join(base, name)\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    return path\n","work_dir = mkdir('exp', 'brs')\n","monitor_dir = mkdir(work_dir, 'monitor')\n","max_episode_steps = env._max_episode_steps\n","save_env_vid = False\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31n5eb03p-Fm"},"source":["## We initialize the variables"]},{"cell_type":"code","metadata":{"id":"1vN5EvxK_QhT","executionInfo":{"status":"ok","timestamp":1671226986674,"user_tz":-540,"elapsed":4,"user":{"displayName":"최세훈","userId":"00148754616075165078"}}},"source":["total_timesteps = 0\n","timesteps_since_eval = 0\n","episode_num = 0\n","done = True\n","t0 = time.time()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9gsjvtPqLgT"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"y_ouY4NH_Y0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671232129295,"user_tz":-540,"elapsed":5142624,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"e8b01862-6fb8-46db-f004-233709558120"},"source":["# We start the main loop over 500,000 timesteps\n","while total_timesteps < max_timesteps:\n","  \n","  # If the episode is done\n","  if done:\n","\n","    # If we are not at the very beginning, we start the training process of the model\n","    if total_timesteps != 0:\n","      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n","      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n","\n","    # We evaluate the episode and we save the policy\n","    if timesteps_since_eval >= eval_freq:\n","      timesteps_since_eval %= eval_freq\n","      evaluations.append(evaluate_policy(policy))\n","      policy.save(file_name, directory=\"./pytorch_models\")\n","      np.save(\"./results/%s\" % (file_name), evaluations)\n","    \n","    # When the training step is done, we reset the state of the environment\n","    obs = env.reset()\n","    \n","    # Set the Done to False\n","    done = False\n","    \n","    # Set rewards and episode timesteps to zero\n","    episode_reward = 0\n","    episode_timesteps = 0\n","    episode_num += 1\n","  \n","  # Before 10000 timesteps, we play random actions\n","  if total_timesteps < start_timesteps:\n","    action = env.action_space.sample()\n","  else: # After 10000 timesteps, we switch to the model\n","    action = policy.select_action(np.array(obs))\n","    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n","    if expl_noise != 0:\n","      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n","  \n","  # The agent performs the action in the environment, then reaches the next state and receives the reward\n","  new_obs, reward, done, _ = env.step(action)\n","  \n","  # We check if the episode is done\n","  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n","  \n","  # We increase the total reward\n","  episode_reward += reward\n","  \n","  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n","  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n","\n","  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n","  obs = new_obs\n","  episode_timesteps += 1\n","  total_timesteps += 1\n","  timesteps_since_eval += 1\n","\n","# We add the last policy evaluation to our list of evaluations and we save our model\n","evaluations.append(evaluate_policy(policy))\n","if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n","np.save(\"./results/%s\" % (file_name), evaluations)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Timesteps: 104 Episode Num: 1 Reward: 48.215434011221795\n","Total Timesteps: 1104 Episode Num: 2 Reward: 489.89134643083554\n","Total Timesteps: 2104 Episode Num: 3 Reward: 482.1018101381439\n","Total Timesteps: 2124 Episode Num: 4 Reward: 5.471657535196574\n","Total Timesteps: 3124 Episode Num: 5 Reward: 519.2660662954889\n","Total Timesteps: 4124 Episode Num: 6 Reward: 509.0569714511031\n","Total Timesteps: 4668 Episode Num: 7 Reward: 265.9056285617177\n","Total Timesteps: 5668 Episode Num: 8 Reward: 473.4490693081326\n","---------------------------------------\n","Average Reward over the Evaluation Step: 250.228216\n","---------------------------------------\n","Total Timesteps: 6668 Episode Num: 9 Reward: 507.47407768773843\n","Total Timesteps: 7668 Episode Num: 10 Reward: 481.92832353754153\n","Total Timesteps: 8668 Episode Num: 11 Reward: 492.68798378771754\n","Total Timesteps: 9668 Episode Num: 12 Reward: 502.7159089288333\n","Total Timesteps: 10668 Episode Num: 13 Reward: 581.0734949217014\n","---------------------------------------\n","Average Reward over the Evaluation Step: 134.021714\n","---------------------------------------\n","Total Timesteps: 11668 Episode Num: 14 Reward: 117.75929014652995\n","Total Timesteps: 12668 Episode Num: 15 Reward: 309.2468746705358\n","Total Timesteps: 12784 Episode Num: 16 Reward: -36.4439371063865\n","Total Timesteps: 13784 Episode Num: 17 Reward: 486.4146320247625\n","Total Timesteps: 13866 Episode Num: 18 Reward: -18.160711867994724\n","Total Timesteps: 14866 Episode Num: 19 Reward: 236.97142637129457\n","Total Timesteps: 15866 Episode Num: 20 Reward: 145.4959427756161\n","---------------------------------------\n","Average Reward over the Evaluation Step: 256.573069\n","---------------------------------------\n","Total Timesteps: 16866 Episode Num: 21 Reward: 341.8588290877263\n","Total Timesteps: 17866 Episode Num: 22 Reward: 238.77391561868242\n","Total Timesteps: 18866 Episode Num: 23 Reward: 191.222516965634\n","Total Timesteps: 19866 Episode Num: 24 Reward: 225.22665241964918\n","Total Timesteps: 20032 Episode Num: 25 Reward: -4.342136999159681\n","---------------------------------------\n","Average Reward over the Evaluation Step: 75.532149\n","---------------------------------------\n","Total Timesteps: 21032 Episode Num: 26 Reward: 95.47937476399757\n","Total Timesteps: 21748 Episode Num: 27 Reward: 139.79260215089755\n","Total Timesteps: 21768 Episode Num: 28 Reward: 1.9685099240002515\n","Total Timesteps: 21788 Episode Num: 29 Reward: 1.570918051947853\n","Total Timesteps: 21808 Episode Num: 30 Reward: 1.089389575803279\n","Total Timesteps: 21828 Episode Num: 31 Reward: 1.4607763563883567\n","Total Timesteps: 21848 Episode Num: 32 Reward: 1.6165384738389115\n","Total Timesteps: 21868 Episode Num: 33 Reward: 2.1759901901303245\n","Total Timesteps: 21888 Episode Num: 34 Reward: 2.171878301912361\n","Total Timesteps: 21908 Episode Num: 35 Reward: 2.7999302064327605\n","Total Timesteps: 21928 Episode Num: 36 Reward: 2.5953776492719838\n","Total Timesteps: 21948 Episode Num: 37 Reward: 3.2136764354650342\n","Total Timesteps: 22948 Episode Num: 38 Reward: 433.6059071860089\n","Total Timesteps: 23948 Episode Num: 39 Reward: 98.56425342989455\n","Total Timesteps: 24948 Episode Num: 40 Reward: 104.01341264310425\n","Total Timesteps: 25948 Episode Num: 41 Reward: 96.43950517191541\n","---------------------------------------\n","Average Reward over the Evaluation Step: 315.106762\n","---------------------------------------\n","Total Timesteps: 26948 Episode Num: 42 Reward: 404.645008768843\n","Total Timesteps: 27948 Episode Num: 43 Reward: 221.9602281964688\n","Total Timesteps: 28948 Episode Num: 44 Reward: 251.79152244650282\n","Total Timesteps: 29948 Episode Num: 45 Reward: 175.5408966917346\n","Total Timesteps: 30948 Episode Num: 46 Reward: 223.43277999736182\n","---------------------------------------\n","Average Reward over the Evaluation Step: 229.138005\n","---------------------------------------\n","Total Timesteps: 31948 Episode Num: 47 Reward: 221.90259265285317\n","Total Timesteps: 32948 Episode Num: 48 Reward: 364.5751490589544\n","Total Timesteps: 33948 Episode Num: 49 Reward: 432.29799905887126\n","Total Timesteps: 34948 Episode Num: 50 Reward: 579.7460536806803\n","Total Timesteps: 35948 Episode Num: 51 Reward: 428.8415234867033\n","---------------------------------------\n","Average Reward over the Evaluation Step: 471.318762\n","---------------------------------------\n","Total Timesteps: 36948 Episode Num: 52 Reward: 675.5271413933854\n","Total Timesteps: 37948 Episode Num: 53 Reward: 593.4696546313761\n","Total Timesteps: 38948 Episode Num: 54 Reward: 175.4355038891752\n","Total Timesteps: 39948 Episode Num: 55 Reward: 445.2015699038611\n","Total Timesteps: 40948 Episode Num: 56 Reward: 327.90926333631944\n","---------------------------------------\n","Average Reward over the Evaluation Step: 364.593711\n","---------------------------------------\n","Total Timesteps: 41948 Episode Num: 57 Reward: 365.6351980209384\n","Total Timesteps: 42948 Episode Num: 58 Reward: 322.68626567409945\n","Total Timesteps: 43948 Episode Num: 59 Reward: 528.9006347014132\n","Total Timesteps: 44948 Episode Num: 60 Reward: 529.1287350300828\n","Total Timesteps: 45948 Episode Num: 61 Reward: 350.70323220646634\n","---------------------------------------\n","Average Reward over the Evaluation Step: 564.620114\n","---------------------------------------\n","Total Timesteps: 46948 Episode Num: 62 Reward: 502.8401851963048\n","Total Timesteps: 47948 Episode Num: 63 Reward: 556.1892427494513\n","Total Timesteps: 48948 Episode Num: 64 Reward: 419.89823770992393\n","Total Timesteps: 49948 Episode Num: 65 Reward: 175.51389494701928\n","Total Timesteps: 50948 Episode Num: 66 Reward: 499.0503728588115\n","---------------------------------------\n","Average Reward over the Evaluation Step: 515.983936\n","---------------------------------------\n","Total Timesteps: 51948 Episode Num: 67 Reward: 639.560696899043\n","Total Timesteps: 52948 Episode Num: 68 Reward: 445.96462153281624\n","Total Timesteps: 53948 Episode Num: 69 Reward: 585.433484019947\n","Total Timesteps: 54948 Episode Num: 70 Reward: 549.9184633826703\n","Total Timesteps: 55948 Episode Num: 71 Reward: 568.0431224903678\n","---------------------------------------\n","Average Reward over the Evaluation Step: 475.535730\n","---------------------------------------\n","Total Timesteps: 56948 Episode Num: 72 Reward: 495.5401249010964\n","Total Timesteps: 57948 Episode Num: 73 Reward: 635.548274146227\n","Total Timesteps: 58948 Episode Num: 74 Reward: 543.7365883840421\n","Total Timesteps: 59948 Episode Num: 75 Reward: 437.09107750712747\n","Total Timesteps: 60948 Episode Num: 76 Reward: 550.5079677743853\n","---------------------------------------\n","Average Reward over the Evaluation Step: 340.462386\n","---------------------------------------\n","Total Timesteps: 61948 Episode Num: 77 Reward: 148.81196926536816\n","Total Timesteps: 62948 Episode Num: 78 Reward: 334.85570916484346\n","Total Timesteps: 63948 Episode Num: 79 Reward: 468.2180546729642\n","Total Timesteps: 64948 Episode Num: 80 Reward: 390.9023993841007\n","Total Timesteps: 65948 Episode Num: 81 Reward: 372.0252232860347\n","---------------------------------------\n","Average Reward over the Evaluation Step: 500.808084\n","---------------------------------------\n","Total Timesteps: 66948 Episode Num: 82 Reward: 368.3704023071842\n","Total Timesteps: 67948 Episode Num: 83 Reward: 613.6711391944038\n","Total Timesteps: 68948 Episode Num: 84 Reward: 446.20629043866495\n","Total Timesteps: 69948 Episode Num: 85 Reward: 555.1893318897465\n","Total Timesteps: 70948 Episode Num: 86 Reward: 321.01503890312864\n","---------------------------------------\n","Average Reward over the Evaluation Step: 412.914847\n","---------------------------------------\n","Total Timesteps: 71948 Episode Num: 87 Reward: 652.8738900883749\n","Total Timesteps: 72948 Episode Num: 88 Reward: 622.824842460956\n","Total Timesteps: 73948 Episode Num: 89 Reward: 376.8371765581233\n","Total Timesteps: 74948 Episode Num: 90 Reward: 658.0953862732\n","Total Timesteps: 75948 Episode Num: 91 Reward: 791.8499437676335\n","---------------------------------------\n","Average Reward over the Evaluation Step: 588.533585\n","---------------------------------------\n","Total Timesteps: 76927 Episode Num: 92 Reward: 600.2597414155998\n","Total Timesteps: 77927 Episode Num: 93 Reward: 677.5726565820166\n","Total Timesteps: 78927 Episode Num: 94 Reward: 254.69596814261496\n","Total Timesteps: 79927 Episode Num: 95 Reward: 517.5546238556894\n","Total Timesteps: 80927 Episode Num: 96 Reward: 487.5707921323649\n","---------------------------------------\n","Average Reward over the Evaluation Step: 490.271706\n","---------------------------------------\n","Total Timesteps: 81386 Episode Num: 97 Reward: 273.1331742556915\n","Total Timesteps: 82386 Episode Num: 98 Reward: 561.8886071343032\n","Total Timesteps: 83386 Episode Num: 99 Reward: 531.4606803138912\n","Total Timesteps: 84386 Episode Num: 100 Reward: 304.35928050034306\n","Total Timesteps: 85310 Episode Num: 101 Reward: 325.7501648863368\n","---------------------------------------\n","Average Reward over the Evaluation Step: 477.821397\n","---------------------------------------\n","Total Timesteps: 86310 Episode Num: 102 Reward: 661.5793955723784\n","Total Timesteps: 87310 Episode Num: 103 Reward: 282.19929269678727\n","Total Timesteps: 88310 Episode Num: 104 Reward: 644.7802254814046\n","Total Timesteps: 89310 Episode Num: 105 Reward: 737.2819173439051\n","Total Timesteps: 90310 Episode Num: 106 Reward: 515.6695222997389\n","---------------------------------------\n","Average Reward over the Evaluation Step: 325.854577\n","---------------------------------------\n","Total Timesteps: 91310 Episode Num: 107 Reward: 237.37631893504752\n","Total Timesteps: 92310 Episode Num: 108 Reward: 407.47014289570484\n","Total Timesteps: 93310 Episode Num: 109 Reward: 549.9436889297517\n","Total Timesteps: 94310 Episode Num: 110 Reward: 546.5373328547414\n","Total Timesteps: 95310 Episode Num: 111 Reward: 718.7857670071757\n","---------------------------------------\n","Average Reward over the Evaluation Step: 634.548897\n","---------------------------------------\n","Total Timesteps: 96310 Episode Num: 112 Reward: 213.5135297944092\n","Total Timesteps: 97310 Episode Num: 113 Reward: 421.00006965210673\n","Total Timesteps: 98310 Episode Num: 114 Reward: 456.34816906891564\n","Total Timesteps: 99310 Episode Num: 115 Reward: 575.1464933491562\n","Total Timesteps: 100310 Episode Num: 116 Reward: 698.8046865066932\n","---------------------------------------\n","Average Reward over the Evaluation Step: 519.401530\n","---------------------------------------\n","Total Timesteps: 101310 Episode Num: 117 Reward: 398.8406235175833\n","Total Timesteps: 102310 Episode Num: 118 Reward: 647.3953179033617\n","Total Timesteps: 103310 Episode Num: 119 Reward: 720.6138025189251\n","Total Timesteps: 104310 Episode Num: 120 Reward: 623.2649124250215\n","Total Timesteps: 105310 Episode Num: 121 Reward: 426.65672981818466\n","---------------------------------------\n","Average Reward over the Evaluation Step: 468.450807\n","---------------------------------------\n","Total Timesteps: 106310 Episode Num: 122 Reward: 551.8585545003946\n","Total Timesteps: 107310 Episode Num: 123 Reward: 655.6861304102828\n","Total Timesteps: 108310 Episode Num: 124 Reward: 590.7997663029411\n","Total Timesteps: 109310 Episode Num: 125 Reward: 705.9971170367197\n","Total Timesteps: 110310 Episode Num: 126 Reward: 421.06706471036165\n","---------------------------------------\n","Average Reward over the Evaluation Step: 601.166188\n","---------------------------------------\n","Total Timesteps: 111310 Episode Num: 127 Reward: 484.57055066484253\n","Total Timesteps: 112310 Episode Num: 128 Reward: 510.33238533346616\n","Total Timesteps: 113310 Episode Num: 129 Reward: 431.16458129981146\n","Total Timesteps: 114310 Episode Num: 130 Reward: 633.7607822601135\n","Total Timesteps: 115310 Episode Num: 131 Reward: 453.20910540657184\n","---------------------------------------\n","Average Reward over the Evaluation Step: 596.529224\n","---------------------------------------\n","Total Timesteps: 116310 Episode Num: 132 Reward: 584.297893692879\n","Total Timesteps: 117310 Episode Num: 133 Reward: 551.0202938386423\n","Total Timesteps: 118310 Episode Num: 134 Reward: 737.8994339146733\n","Total Timesteps: 119310 Episode Num: 135 Reward: 942.8556395291909\n","Total Timesteps: 120310 Episode Num: 136 Reward: 753.1160489491106\n","---------------------------------------\n","Average Reward over the Evaluation Step: 739.735008\n","---------------------------------------\n","Total Timesteps: 121310 Episode Num: 137 Reward: 775.1837192417449\n","Total Timesteps: 122310 Episode Num: 138 Reward: 706.4710339162027\n","Total Timesteps: 123310 Episode Num: 139 Reward: 589.6906776437067\n","Total Timesteps: 124310 Episode Num: 140 Reward: 682.5466053381863\n","Total Timesteps: 125310 Episode Num: 141 Reward: 771.0066509556847\n","---------------------------------------\n","Average Reward over the Evaluation Step: 769.561227\n","---------------------------------------\n","Total Timesteps: 126310 Episode Num: 142 Reward: 874.7550688038623\n","Total Timesteps: 127310 Episode Num: 143 Reward: 754.116161845124\n","Total Timesteps: 128310 Episode Num: 144 Reward: 807.4091168858553\n","Total Timesteps: 129310 Episode Num: 145 Reward: 575.3682253334925\n","Total Timesteps: 130310 Episode Num: 146 Reward: 431.35938020675235\n","---------------------------------------\n","Average Reward over the Evaluation Step: 739.211801\n","---------------------------------------\n","Total Timesteps: 131310 Episode Num: 147 Reward: 630.0538124611137\n","Total Timesteps: 132310 Episode Num: 148 Reward: 716.2701834774167\n","Total Timesteps: 133310 Episode Num: 149 Reward: 693.1190429599797\n","Total Timesteps: 134310 Episode Num: 150 Reward: 758.2377871856829\n","Total Timesteps: 135310 Episode Num: 151 Reward: 591.7557985096636\n","---------------------------------------\n","Average Reward over the Evaluation Step: 712.217208\n","---------------------------------------\n","Total Timesteps: 136310 Episode Num: 152 Reward: 528.7841326701316\n","Total Timesteps: 137310 Episode Num: 153 Reward: 752.3849160964403\n","Total Timesteps: 138310 Episode Num: 154 Reward: 796.3508279573571\n","Total Timesteps: 139310 Episode Num: 155 Reward: 855.402096216797\n","Total Timesteps: 140310 Episode Num: 156 Reward: 880.8798697691124\n","---------------------------------------\n","Average Reward over the Evaluation Step: 816.834657\n","---------------------------------------\n","Total Timesteps: 141310 Episode Num: 157 Reward: 756.7113298338891\n","Total Timesteps: 142310 Episode Num: 158 Reward: 723.6031973219046\n","Total Timesteps: 143310 Episode Num: 159 Reward: 820.0365696686404\n","Total Timesteps: 144310 Episode Num: 160 Reward: 548.1444176596648\n","Total Timesteps: 145310 Episode Num: 161 Reward: 828.8022732670394\n","---------------------------------------\n","Average Reward over the Evaluation Step: 826.636637\n","---------------------------------------\n","Total Timesteps: 146310 Episode Num: 162 Reward: 748.9103982128864\n","Total Timesteps: 147310 Episode Num: 163 Reward: 677.7630662918955\n","Total Timesteps: 148310 Episode Num: 164 Reward: 561.0501037887444\n","Total Timesteps: 149310 Episode Num: 165 Reward: 858.8107621367627\n","Total Timesteps: 150310 Episode Num: 166 Reward: 889.2593453469774\n","---------------------------------------\n","Average Reward over the Evaluation Step: 684.048436\n","---------------------------------------\n","Total Timesteps: 151310 Episode Num: 167 Reward: 782.3552041958716\n","Total Timesteps: 152310 Episode Num: 168 Reward: 752.1501651093007\n","Total Timesteps: 153310 Episode Num: 169 Reward: 594.5135495155826\n","Total Timesteps: 154310 Episode Num: 170 Reward: 761.695598144691\n","Total Timesteps: 155310 Episode Num: 171 Reward: 732.7806157349136\n","---------------------------------------\n","Average Reward over the Evaluation Step: 683.030741\n","---------------------------------------\n","Total Timesteps: 156310 Episode Num: 172 Reward: 727.3659035981384\n","Total Timesteps: 157310 Episode Num: 173 Reward: 546.1623343733645\n","Total Timesteps: 158310 Episode Num: 174 Reward: 397.35379135095155\n","Total Timesteps: 159310 Episode Num: 175 Reward: 334.81066059236576\n","Total Timesteps: 160310 Episode Num: 176 Reward: 685.9732671061903\n","---------------------------------------\n","Average Reward over the Evaluation Step: 700.151805\n","---------------------------------------\n","Total Timesteps: 161310 Episode Num: 177 Reward: 558.5504818737071\n","Total Timesteps: 162310 Episode Num: 178 Reward: 827.6947048173871\n","Total Timesteps: 163310 Episode Num: 179 Reward: 764.1552355992361\n","Total Timesteps: 164310 Episode Num: 180 Reward: 561.1831284821832\n","Total Timesteps: 165310 Episode Num: 181 Reward: 668.755057914816\n","---------------------------------------\n","Average Reward over the Evaluation Step: 737.278298\n","---------------------------------------\n","Total Timesteps: 166310 Episode Num: 182 Reward: 702.9210403557244\n","Total Timesteps: 167310 Episode Num: 183 Reward: 696.324730498237\n","Total Timesteps: 168310 Episode Num: 184 Reward: 590.0125297942739\n","Total Timesteps: 169310 Episode Num: 185 Reward: 779.640555606713\n","Total Timesteps: 170310 Episode Num: 186 Reward: 812.7402040854696\n","---------------------------------------\n","Average Reward over the Evaluation Step: 737.288372\n","---------------------------------------\n","Total Timesteps: 171310 Episode Num: 187 Reward: 830.3692886596505\n","Total Timesteps: 172310 Episode Num: 188 Reward: 835.6139388978339\n","Total Timesteps: 173310 Episode Num: 189 Reward: 960.3766900522512\n","Total Timesteps: 174310 Episode Num: 190 Reward: 588.1027031349856\n","Total Timesteps: 175310 Episode Num: 191 Reward: 754.4604916808532\n","---------------------------------------\n","Average Reward over the Evaluation Step: 714.904669\n","---------------------------------------\n","Total Timesteps: 176310 Episode Num: 192 Reward: 547.154557813645\n","Total Timesteps: 177310 Episode Num: 193 Reward: 634.754636316042\n","Total Timesteps: 178310 Episode Num: 194 Reward: 987.5787896251669\n","Total Timesteps: 179310 Episode Num: 195 Reward: 762.4556057414093\n","Total Timesteps: 180310 Episode Num: 196 Reward: 754.2249429117554\n","---------------------------------------\n","Average Reward over the Evaluation Step: 652.316598\n","---------------------------------------\n","Total Timesteps: 181310 Episode Num: 197 Reward: 647.426664310298\n","Total Timesteps: 182310 Episode Num: 198 Reward: 687.6290802489519\n","Total Timesteps: 183310 Episode Num: 199 Reward: 797.0336210070765\n","Total Timesteps: 184310 Episode Num: 200 Reward: 859.5056861602774\n","Total Timesteps: 185310 Episode Num: 201 Reward: 988.7105945848697\n","---------------------------------------\n","Average Reward over the Evaluation Step: 728.850166\n","---------------------------------------\n","Total Timesteps: 186310 Episode Num: 202 Reward: 851.5258512573585\n","Total Timesteps: 187310 Episode Num: 203 Reward: 856.928037572877\n","Total Timesteps: 188310 Episode Num: 204 Reward: 592.1264181353916\n","Total Timesteps: 189310 Episode Num: 205 Reward: 624.7817644157458\n","Total Timesteps: 190310 Episode Num: 206 Reward: 832.2367081751897\n","---------------------------------------\n","Average Reward over the Evaluation Step: 910.197413\n","---------------------------------------\n","Total Timesteps: 191310 Episode Num: 207 Reward: 907.3712515932199\n","Total Timesteps: 192310 Episode Num: 208 Reward: 959.1441467326814\n","Total Timesteps: 193310 Episode Num: 209 Reward: 906.1645372317521\n","Total Timesteps: 194310 Episode Num: 210 Reward: 1092.7166018125333\n","Total Timesteps: 195310 Episode Num: 211 Reward: 1144.282519877046\n","---------------------------------------\n","Average Reward over the Evaluation Step: 842.524020\n","---------------------------------------\n","Total Timesteps: 196310 Episode Num: 212 Reward: 592.0422170458661\n","Total Timesteps: 197310 Episode Num: 213 Reward: 860.2353581857799\n","Total Timesteps: 198310 Episode Num: 214 Reward: 911.9549307520941\n","Total Timesteps: 199310 Episode Num: 215 Reward: 908.3241001590264\n","Total Timesteps: 200310 Episode Num: 216 Reward: 835.1140895889462\n","---------------------------------------\n","Average Reward over the Evaluation Step: 834.340038\n","---------------------------------------\n","Total Timesteps: 201310 Episode Num: 217 Reward: 808.5176961677499\n","Total Timesteps: 202310 Episode Num: 218 Reward: 1240.1062268183123\n","Total Timesteps: 203310 Episode Num: 219 Reward: 1070.1771349890494\n","Total Timesteps: 204310 Episode Num: 220 Reward: 766.4168154794479\n","Total Timesteps: 205310 Episode Num: 221 Reward: 845.7011425845744\n","---------------------------------------\n","Average Reward over the Evaluation Step: 870.485414\n","---------------------------------------\n","Total Timesteps: 206310 Episode Num: 222 Reward: 761.5720265999624\n","Total Timesteps: 207310 Episode Num: 223 Reward: 709.3246720020328\n","Total Timesteps: 208310 Episode Num: 224 Reward: 906.0748647788806\n","Total Timesteps: 209310 Episode Num: 225 Reward: 850.8703061050978\n","Total Timesteps: 210310 Episode Num: 226 Reward: 686.973033106972\n","---------------------------------------\n","Average Reward over the Evaluation Step: 855.380404\n","---------------------------------------\n","Total Timesteps: 211310 Episode Num: 227 Reward: 1013.7742384590955\n","Total Timesteps: 212310 Episode Num: 228 Reward: 581.0157941144148\n","Total Timesteps: 213310 Episode Num: 229 Reward: 830.5425890900963\n","Total Timesteps: 214310 Episode Num: 230 Reward: 937.3233618431439\n","Total Timesteps: 215310 Episode Num: 231 Reward: 750.9163550866629\n","---------------------------------------\n","Average Reward over the Evaluation Step: 856.720936\n","---------------------------------------\n","Total Timesteps: 216310 Episode Num: 232 Reward: 1014.1329188970835\n","Total Timesteps: 217310 Episode Num: 233 Reward: 1027.4406444189542\n","Total Timesteps: 218310 Episode Num: 234 Reward: 819.3408050924178\n","Total Timesteps: 219310 Episode Num: 235 Reward: 1391.7890161616428\n","Total Timesteps: 220310 Episode Num: 236 Reward: 1260.3834332740523\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1171.967336\n","---------------------------------------\n","Total Timesteps: 221310 Episode Num: 237 Reward: 1247.6989835438364\n","Total Timesteps: 222310 Episode Num: 238 Reward: 1230.549137342326\n","Total Timesteps: 223310 Episode Num: 239 Reward: 919.6997152388375\n","Total Timesteps: 224310 Episode Num: 240 Reward: 1217.254345405113\n","Total Timesteps: 225310 Episode Num: 241 Reward: 1345.090562444647\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1210.241419\n","---------------------------------------\n","Total Timesteps: 226310 Episode Num: 242 Reward: 1307.9880315606713\n","Total Timesteps: 227310 Episode Num: 243 Reward: 955.6274135488499\n","Total Timesteps: 228310 Episode Num: 244 Reward: 679.5173177356935\n","Total Timesteps: 229310 Episode Num: 245 Reward: 1230.5635406772108\n","Total Timesteps: 230310 Episode Num: 246 Reward: 868.2848712529608\n","---------------------------------------\n","Average Reward over the Evaluation Step: 911.173321\n","---------------------------------------\n","Total Timesteps: 231310 Episode Num: 247 Reward: 878.2365839765503\n","Total Timesteps: 232310 Episode Num: 248 Reward: 1458.4052866055522\n","Total Timesteps: 233310 Episode Num: 249 Reward: 769.1158093551217\n","Total Timesteps: 234310 Episode Num: 250 Reward: 890.8666223446324\n","Total Timesteps: 235310 Episode Num: 251 Reward: 1033.02835420933\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1260.652700\n","---------------------------------------\n","Total Timesteps: 236310 Episode Num: 252 Reward: 1134.757316135406\n","Total Timesteps: 237310 Episode Num: 253 Reward: 965.1993957067833\n","Total Timesteps: 238310 Episode Num: 254 Reward: 771.4228914187955\n","Total Timesteps: 239310 Episode Num: 255 Reward: 639.3384010550521\n","Total Timesteps: 240310 Episode Num: 256 Reward: 1383.1814098112372\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1412.472782\n","---------------------------------------\n","Total Timesteps: 241310 Episode Num: 257 Reward: 1388.9418727381121\n","Total Timesteps: 242310 Episode Num: 258 Reward: 1456.4275371474275\n","Total Timesteps: 243310 Episode Num: 259 Reward: 1461.5881795019875\n","Total Timesteps: 244310 Episode Num: 260 Reward: 1465.9675414385756\n","Total Timesteps: 245310 Episode Num: 261 Reward: 1468.5421162442494\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1114.390144\n","---------------------------------------\n","Total Timesteps: 246310 Episode Num: 262 Reward: 1047.8583375907651\n","Total Timesteps: 247310 Episode Num: 263 Reward: 1243.774026117605\n","Total Timesteps: 248310 Episode Num: 264 Reward: 1536.2244496282174\n","Total Timesteps: 249310 Episode Num: 265 Reward: 1012.6462410519131\n","Total Timesteps: 250310 Episode Num: 266 Reward: 1056.1970658803928\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1516.531145\n","---------------------------------------\n","Total Timesteps: 251310 Episode Num: 267 Reward: 1654.6464707332989\n","Total Timesteps: 252310 Episode Num: 268 Reward: 1719.0303459374418\n","Total Timesteps: 253310 Episode Num: 269 Reward: 1637.3221426226332\n","Total Timesteps: 254310 Episode Num: 270 Reward: 1614.6562248701584\n","Total Timesteps: 255310 Episode Num: 271 Reward: 1629.4785642890477\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1298.395939\n","---------------------------------------\n","Total Timesteps: 256310 Episode Num: 272 Reward: 1303.9444518094765\n","Total Timesteps: 257310 Episode Num: 273 Reward: 1516.9581901459007\n","Total Timesteps: 258310 Episode Num: 274 Reward: 1513.4250105028386\n","Total Timesteps: 259310 Episode Num: 275 Reward: 1587.41276927382\n","Total Timesteps: 260310 Episode Num: 276 Reward: 1575.7737736292459\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1522.247605\n","---------------------------------------\n","Total Timesteps: 261310 Episode Num: 277 Reward: 1513.732205187984\n","Total Timesteps: 262310 Episode Num: 278 Reward: 1264.2238557382784\n","Total Timesteps: 263310 Episode Num: 279 Reward: 895.4119200391405\n","Total Timesteps: 264310 Episode Num: 280 Reward: 1414.6267116563586\n","Total Timesteps: 265310 Episode Num: 281 Reward: 1146.3438940645033\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1359.878099\n","---------------------------------------\n","Total Timesteps: 266310 Episode Num: 282 Reward: 1287.0912740700367\n","Total Timesteps: 267310 Episode Num: 283 Reward: 1671.5186222856537\n","Total Timesteps: 268310 Episode Num: 284 Reward: 1565.879571717481\n","Total Timesteps: 269310 Episode Num: 285 Reward: 1693.3493118156812\n","Total Timesteps: 270310 Episode Num: 286 Reward: 1488.8215122607385\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1729.265815\n","---------------------------------------\n","Total Timesteps: 271310 Episode Num: 287 Reward: 1712.3559204774951\n","Total Timesteps: 272310 Episode Num: 288 Reward: 1679.6932247705197\n","Total Timesteps: 273310 Episode Num: 289 Reward: 1389.842550415756\n","Total Timesteps: 274310 Episode Num: 290 Reward: 1703.9986951209771\n","Total Timesteps: 275310 Episode Num: 291 Reward: 1785.721724561216\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1620.001030\n","---------------------------------------\n","Total Timesteps: 276310 Episode Num: 292 Reward: 1585.2768701152281\n","Total Timesteps: 277310 Episode Num: 293 Reward: 1687.008253091438\n","Total Timesteps: 278310 Episode Num: 294 Reward: 1996.7041018349817\n","Total Timesteps: 279310 Episode Num: 295 Reward: 1867.2493610257638\n","Total Timesteps: 280310 Episode Num: 296 Reward: 1877.7039675082406\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1907.714307\n","---------------------------------------\n","Total Timesteps: 281310 Episode Num: 297 Reward: 1902.3271189649145\n","Total Timesteps: 282310 Episode Num: 298 Reward: 1983.3250424242738\n","Total Timesteps: 283310 Episode Num: 299 Reward: 1989.5436589073595\n","Total Timesteps: 284310 Episode Num: 300 Reward: 1967.2037607470731\n","Total Timesteps: 285310 Episode Num: 301 Reward: 1948.546143091517\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1938.010439\n","---------------------------------------\n","Total Timesteps: 286310 Episode Num: 302 Reward: 1910.9657454126857\n","Total Timesteps: 287310 Episode Num: 303 Reward: 1965.945918855257\n","Total Timesteps: 288310 Episode Num: 304 Reward: 1954.837859616414\n","Total Timesteps: 289310 Episode Num: 305 Reward: 2000.666201695615\n","Total Timesteps: 290310 Episode Num: 306 Reward: 1886.8594610269117\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2073.109768\n","---------------------------------------\n","Total Timesteps: 291310 Episode Num: 307 Reward: 2031.765788546925\n","Total Timesteps: 292310 Episode Num: 308 Reward: 1838.8846665183607\n","Total Timesteps: 293310 Episode Num: 309 Reward: 1989.6250298685566\n","Total Timesteps: 294310 Episode Num: 310 Reward: 1811.154392697437\n","Total Timesteps: 295310 Episode Num: 311 Reward: 2035.1996257059927\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1935.011343\n","---------------------------------------\n","Total Timesteps: 296310 Episode Num: 312 Reward: 1936.651575390994\n","Total Timesteps: 297310 Episode Num: 313 Reward: 2012.6236219975508\n","Total Timesteps: 298310 Episode Num: 314 Reward: 2020.1757321428468\n","Total Timesteps: 299310 Episode Num: 315 Reward: 1933.556739721154\n","Total Timesteps: 300310 Episode Num: 316 Reward: 1768.0026938930082\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2104.420674\n","---------------------------------------\n","Total Timesteps: 301310 Episode Num: 317 Reward: 2044.7076969182801\n","Total Timesteps: 302310 Episode Num: 318 Reward: 2053.966643079594\n","Total Timesteps: 303310 Episode Num: 319 Reward: 2078.9421957790773\n","Total Timesteps: 304310 Episode Num: 320 Reward: 2076.401216684808\n","Total Timesteps: 305310 Episode Num: 321 Reward: 2126.499387632527\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1988.036746\n","---------------------------------------\n","Total Timesteps: 306310 Episode Num: 322 Reward: 2039.23439339177\n","Total Timesteps: 307310 Episode Num: 323 Reward: 2109.4326158888025\n","Total Timesteps: 308310 Episode Num: 324 Reward: 1904.4568063001832\n","Total Timesteps: 309310 Episode Num: 325 Reward: 1836.6598238695144\n","Total Timesteps: 310310 Episode Num: 326 Reward: 2067.7088286357816\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2103.091055\n","---------------------------------------\n","Total Timesteps: 311310 Episode Num: 327 Reward: 2079.3786720243907\n","Total Timesteps: 312310 Episode Num: 328 Reward: 2121.9715040349197\n","Total Timesteps: 313310 Episode Num: 329 Reward: 2031.7876511264742\n","Total Timesteps: 314310 Episode Num: 330 Reward: 1820.051284425437\n","Total Timesteps: 315310 Episode Num: 331 Reward: 2099.9567853104795\n","---------------------------------------\n","Average Reward over the Evaluation Step: 1958.840746\n","---------------------------------------\n","Total Timesteps: 316310 Episode Num: 332 Reward: 2009.3576105817924\n","Total Timesteps: 317310 Episode Num: 333 Reward: 2109.0987771640753\n","Total Timesteps: 318310 Episode Num: 334 Reward: 2034.0251713232701\n","Total Timesteps: 319310 Episode Num: 335 Reward: 2164.5269455310677\n","Total Timesteps: 320310 Episode Num: 336 Reward: 2021.5243566250201\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2180.569650\n","---------------------------------------\n","Total Timesteps: 321310 Episode Num: 337 Reward: 2232.1106140907027\n","Total Timesteps: 322310 Episode Num: 338 Reward: 2207.275607163685\n","Total Timesteps: 323310 Episode Num: 339 Reward: 2133.9655341936364\n","Total Timesteps: 324310 Episode Num: 340 Reward: 2140.044066268212\n","Total Timesteps: 325310 Episode Num: 341 Reward: 2164.304169763663\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2068.464481\n","---------------------------------------\n","Total Timesteps: 326310 Episode Num: 342 Reward: 2020.7120478701795\n","Total Timesteps: 327310 Episode Num: 343 Reward: 2234.627123059853\n","Total Timesteps: 328310 Episode Num: 344 Reward: 2119.3682338265808\n","Total Timesteps: 329310 Episode Num: 345 Reward: 2071.468493937807\n","Total Timesteps: 330310 Episode Num: 346 Reward: 2197.764768223658\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2138.091137\n","---------------------------------------\n","Total Timesteps: 331310 Episode Num: 347 Reward: 2159.5666639877686\n","Total Timesteps: 332310 Episode Num: 348 Reward: 2026.1494034977686\n","Total Timesteps: 333310 Episode Num: 349 Reward: 2218.8509968697463\n","Total Timesteps: 334310 Episode Num: 350 Reward: 2159.157547940051\n","Total Timesteps: 335310 Episode Num: 351 Reward: 2178.0370816582367\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2286.528455\n","---------------------------------------\n","Total Timesteps: 336310 Episode Num: 352 Reward: 2311.333926629341\n","Total Timesteps: 337310 Episode Num: 353 Reward: 2218.0242194741427\n","Total Timesteps: 338310 Episode Num: 354 Reward: 2183.355672040695\n","Total Timesteps: 339310 Episode Num: 355 Reward: 2202.6369271505628\n","Total Timesteps: 340310 Episode Num: 356 Reward: 2101.918209322385\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2283.971857\n","---------------------------------------\n","Total Timesteps: 341310 Episode Num: 357 Reward: 2268.9397056518037\n","Total Timesteps: 342310 Episode Num: 358 Reward: 2020.002390294583\n","Total Timesteps: 343310 Episode Num: 359 Reward: 2235.350298241652\n","Total Timesteps: 344310 Episode Num: 360 Reward: 2264.2338622247635\n","Total Timesteps: 345310 Episode Num: 361 Reward: 2083.83905642886\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2132.581839\n","---------------------------------------\n","Total Timesteps: 346310 Episode Num: 362 Reward: 2132.750247811306\n","Total Timesteps: 347310 Episode Num: 363 Reward: 2225.1957605359567\n","Total Timesteps: 348310 Episode Num: 364 Reward: 2287.328669057999\n","Total Timesteps: 349310 Episode Num: 365 Reward: 2115.895128956268\n","Total Timesteps: 350310 Episode Num: 366 Reward: 2099.0061029179283\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2255.602463\n","---------------------------------------\n","Total Timesteps: 351310 Episode Num: 367 Reward: 2182.9671373034957\n","Total Timesteps: 352310 Episode Num: 368 Reward: 2233.3219145829044\n","Total Timesteps: 353310 Episode Num: 369 Reward: 2196.0798163632803\n","Total Timesteps: 354310 Episode Num: 370 Reward: 2116.3995543666715\n","Total Timesteps: 355310 Episode Num: 371 Reward: 2209.398578237717\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2256.296193\n","---------------------------------------\n","Total Timesteps: 356310 Episode Num: 372 Reward: 2200.9734199884338\n","Total Timesteps: 357310 Episode Num: 373 Reward: 2171.8284090792285\n","Total Timesteps: 358310 Episode Num: 374 Reward: 2190.6861315198635\n","Total Timesteps: 359310 Episode Num: 375 Reward: 2167.2652519675253\n","Total Timesteps: 360310 Episode Num: 376 Reward: 2276.7753454358217\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2294.409294\n","---------------------------------------\n","Total Timesteps: 361310 Episode Num: 377 Reward: 2231.046954757202\n","Total Timesteps: 362310 Episode Num: 378 Reward: 1988.2906975475512\n","Total Timesteps: 363310 Episode Num: 379 Reward: 2116.3640950031067\n","Total Timesteps: 364310 Episode Num: 380 Reward: 2223.06759575135\n","Total Timesteps: 365310 Episode Num: 381 Reward: 2242.974828309862\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2273.404278\n","---------------------------------------\n","Total Timesteps: 366310 Episode Num: 382 Reward: 2236.9817151333446\n","Total Timesteps: 367310 Episode Num: 383 Reward: 2288.2970779154293\n","Total Timesteps: 368310 Episode Num: 384 Reward: 2241.0259680623185\n","Total Timesteps: 369310 Episode Num: 385 Reward: 1922.2625509722855\n","Total Timesteps: 370310 Episode Num: 386 Reward: 2258.5171896139177\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2118.869397\n","---------------------------------------\n","Total Timesteps: 371310 Episode Num: 387 Reward: 2107.9843068728655\n","Total Timesteps: 372310 Episode Num: 388 Reward: 1962.1457682820792\n","Total Timesteps: 373310 Episode Num: 389 Reward: 2341.8759890322394\n","Total Timesteps: 374310 Episode Num: 390 Reward: 2270.4170777837276\n","Total Timesteps: 375310 Episode Num: 391 Reward: 2306.644046869074\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2216.801586\n","---------------------------------------\n","Total Timesteps: 376310 Episode Num: 392 Reward: 2171.4877600354466\n","Total Timesteps: 377310 Episode Num: 393 Reward: 2374.6022560741544\n","Total Timesteps: 378310 Episode Num: 394 Reward: 2278.5117347522814\n","Total Timesteps: 379310 Episode Num: 395 Reward: 2347.4911923882437\n","Total Timesteps: 380310 Episode Num: 396 Reward: 2307.1664253795057\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2241.355359\n","---------------------------------------\n","Total Timesteps: 381310 Episode Num: 397 Reward: 2183.4884360336823\n","Total Timesteps: 382310 Episode Num: 398 Reward: 2227.846382110557\n","Total Timesteps: 383310 Episode Num: 399 Reward: 2299.082942190143\n","Total Timesteps: 384310 Episode Num: 400 Reward: 2381.8464409760763\n","Total Timesteps: 385310 Episode Num: 401 Reward: 2399.7356464778914\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2392.786085\n","---------------------------------------\n","Total Timesteps: 386310 Episode Num: 402 Reward: 2293.4747563068527\n","Total Timesteps: 387310 Episode Num: 403 Reward: 2330.463286603024\n","Total Timesteps: 388310 Episode Num: 404 Reward: 2465.0986509290296\n","Total Timesteps: 389310 Episode Num: 405 Reward: 2419.0760967603405\n","Total Timesteps: 390310 Episode Num: 406 Reward: 2345.827221639362\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2349.266229\n","---------------------------------------\n","Total Timesteps: 391310 Episode Num: 407 Reward: 2275.187575691646\n","Total Timesteps: 392310 Episode Num: 408 Reward: 2293.3959454359915\n","Total Timesteps: 393310 Episode Num: 409 Reward: 2351.650656743783\n","Total Timesteps: 394310 Episode Num: 410 Reward: 2395.3243822587574\n","Total Timesteps: 395310 Episode Num: 411 Reward: 2173.879281038071\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2448.038482\n","---------------------------------------\n","Total Timesteps: 396310 Episode Num: 412 Reward: 2409.621846632812\n","Total Timesteps: 397310 Episode Num: 413 Reward: 2305.2705756338723\n","Total Timesteps: 398310 Episode Num: 414 Reward: 2437.351015637578\n","Total Timesteps: 399310 Episode Num: 415 Reward: 2438.127971537651\n","Total Timesteps: 400310 Episode Num: 416 Reward: 2478.0221905531967\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2395.448056\n","---------------------------------------\n","Total Timesteps: 401310 Episode Num: 417 Reward: 2329.7076039385106\n","Total Timesteps: 402310 Episode Num: 418 Reward: 2297.067467569318\n","Total Timesteps: 403310 Episode Num: 419 Reward: 2407.505994325843\n","Total Timesteps: 404310 Episode Num: 420 Reward: 2392.7513138468735\n","Total Timesteps: 405310 Episode Num: 421 Reward: 2432.996536922218\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2464.118244\n","---------------------------------------\n","Total Timesteps: 406310 Episode Num: 422 Reward: 2361.7917452898964\n","Total Timesteps: 407310 Episode Num: 423 Reward: 2634.7268395946066\n","Total Timesteps: 408310 Episode Num: 424 Reward: 2415.887263720846\n","Total Timesteps: 409310 Episode Num: 425 Reward: 2201.007638992852\n","Total Timesteps: 410310 Episode Num: 426 Reward: 2404.6787358785527\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2448.503257\n","---------------------------------------\n","Total Timesteps: 411310 Episode Num: 427 Reward: 2405.1577144192815\n","Total Timesteps: 412310 Episode Num: 428 Reward: 2436.7362078935084\n","Total Timesteps: 413310 Episode Num: 429 Reward: 2469.092518917758\n","Total Timesteps: 414310 Episode Num: 430 Reward: 2495.2267433205307\n","Total Timesteps: 415310 Episode Num: 431 Reward: 2461.8780555048825\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2446.854590\n","---------------------------------------\n","Total Timesteps: 416310 Episode Num: 432 Reward: 2296.4204617499986\n","Total Timesteps: 417310 Episode Num: 433 Reward: 2488.5362188522536\n","Total Timesteps: 418310 Episode Num: 434 Reward: 2494.234076079989\n","Total Timesteps: 419310 Episode Num: 435 Reward: 2460.7163603762733\n","Total Timesteps: 420310 Episode Num: 436 Reward: 2424.7573605349744\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2367.183097\n","---------------------------------------\n","Total Timesteps: 421310 Episode Num: 437 Reward: 2260.5435938073083\n","Total Timesteps: 422310 Episode Num: 438 Reward: 2430.744297780986\n","Total Timesteps: 423310 Episode Num: 439 Reward: 2386.500900526678\n","Total Timesteps: 424310 Episode Num: 440 Reward: 2425.2024025253327\n","Total Timesteps: 425310 Episode Num: 441 Reward: 2476.6518113494644\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2498.412592\n","---------------------------------------\n","Total Timesteps: 426310 Episode Num: 442 Reward: 2422.46228861713\n","Total Timesteps: 427310 Episode Num: 443 Reward: 2424.1903891221746\n","Total Timesteps: 428310 Episode Num: 444 Reward: 2390.4587289254982\n","Total Timesteps: 429310 Episode Num: 445 Reward: 2428.0064955435437\n","Total Timesteps: 430310 Episode Num: 446 Reward: 2409.1849935103482\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2361.441667\n","---------------------------------------\n","Total Timesteps: 431310 Episode Num: 447 Reward: 2361.835218022478\n","Total Timesteps: 432310 Episode Num: 448 Reward: 2420.4175037636937\n","Total Timesteps: 433310 Episode Num: 449 Reward: 2405.547118583821\n","Total Timesteps: 434310 Episode Num: 450 Reward: 2440.782406683331\n","Total Timesteps: 435310 Episode Num: 451 Reward: 2458.73059112464\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2457.073497\n","---------------------------------------\n","Total Timesteps: 436310 Episode Num: 452 Reward: 2429.8914799233567\n","Total Timesteps: 437310 Episode Num: 453 Reward: 2433.628206364465\n","Total Timesteps: 438310 Episode Num: 454 Reward: 2598.1623595792294\n","Total Timesteps: 439310 Episode Num: 455 Reward: 2510.457882713241\n","Total Timesteps: 440310 Episode Num: 456 Reward: 2380.023186062817\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2512.807847\n","---------------------------------------\n","Total Timesteps: 441310 Episode Num: 457 Reward: 2413.9481539785193\n","Total Timesteps: 442310 Episode Num: 458 Reward: 2397.5615031977063\n","Total Timesteps: 443310 Episode Num: 459 Reward: 2534.116004839263\n","Total Timesteps: 444310 Episode Num: 460 Reward: 2521.9858782123033\n","Total Timesteps: 445310 Episode Num: 461 Reward: 2470.359544809414\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2534.360870\n","---------------------------------------\n","Total Timesteps: 446310 Episode Num: 462 Reward: 2470.421737423059\n","Total Timesteps: 447310 Episode Num: 463 Reward: 2337.349375723523\n","Total Timesteps: 448310 Episode Num: 464 Reward: 2406.026923727305\n","Total Timesteps: 449310 Episode Num: 465 Reward: 2587.2719842676074\n","Total Timesteps: 450310 Episode Num: 466 Reward: 2498.19947448584\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2510.683179\n","---------------------------------------\n","Total Timesteps: 451310 Episode Num: 467 Reward: 2409.970051551441\n","Total Timesteps: 452310 Episode Num: 468 Reward: 2501.744263042295\n","Total Timesteps: 453310 Episode Num: 469 Reward: 2444.6869301724782\n","Total Timesteps: 454310 Episode Num: 470 Reward: 2474.0544879921426\n","Total Timesteps: 455310 Episode Num: 471 Reward: 2335.9397316356926\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2511.549989\n","---------------------------------------\n","Total Timesteps: 456310 Episode Num: 472 Reward: 2513.5566916765274\n","Total Timesteps: 457310 Episode Num: 473 Reward: 2398.3215750276163\n","Total Timesteps: 458310 Episode Num: 474 Reward: 2449.166659371597\n","Total Timesteps: 459310 Episode Num: 475 Reward: 2372.6904108822423\n","Total Timesteps: 460310 Episode Num: 476 Reward: 2411.0405329032496\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2243.915093\n","---------------------------------------\n","Total Timesteps: 461310 Episode Num: 477 Reward: 2349.274844355039\n","Total Timesteps: 462310 Episode Num: 478 Reward: 2381.9160042366007\n","Total Timesteps: 463310 Episode Num: 479 Reward: 2480.8580911256963\n","Total Timesteps: 464310 Episode Num: 480 Reward: 2457.3238132505644\n","Total Timesteps: 465310 Episode Num: 481 Reward: 2548.190297929368\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2563.519286\n","---------------------------------------\n","Total Timesteps: 466310 Episode Num: 482 Reward: 2514.2006986276992\n","Total Timesteps: 467310 Episode Num: 483 Reward: 2499.5494267094423\n","Total Timesteps: 468310 Episode Num: 484 Reward: 2476.5293666332686\n","Total Timesteps: 469310 Episode Num: 485 Reward: 2485.277905502245\n","Total Timesteps: 470310 Episode Num: 486 Reward: 2567.310224569788\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2632.606414\n","---------------------------------------\n","Total Timesteps: 471310 Episode Num: 487 Reward: 2530.1924010326384\n","Total Timesteps: 472310 Episode Num: 488 Reward: 2425.1537720220304\n","Total Timesteps: 473310 Episode Num: 489 Reward: 2473.116327586516\n","Total Timesteps: 474310 Episode Num: 490 Reward: 2571.913792363688\n","Total Timesteps: 475310 Episode Num: 491 Reward: 2628.8145596403488\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2552.025838\n","---------------------------------------\n","Total Timesteps: 476310 Episode Num: 492 Reward: 2561.0094886433562\n","Total Timesteps: 477310 Episode Num: 493 Reward: 2531.381531392028\n","Total Timesteps: 478310 Episode Num: 494 Reward: 2572.1828068406344\n","Total Timesteps: 479310 Episode Num: 495 Reward: 2488.4054389742096\n","Total Timesteps: 480310 Episode Num: 496 Reward: 2562.767289067007\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2639.520354\n","---------------------------------------\n","Total Timesteps: 481310 Episode Num: 497 Reward: 2566.469672571608\n","Total Timesteps: 482310 Episode Num: 498 Reward: 2511.3956086104486\n","Total Timesteps: 483310 Episode Num: 499 Reward: 2482.064012005161\n","Total Timesteps: 484310 Episode Num: 500 Reward: 2474.552982450576\n","Total Timesteps: 485310 Episode Num: 501 Reward: 2541.77241646885\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2422.046850\n","---------------------------------------\n","Total Timesteps: 486310 Episode Num: 502 Reward: 2660.022977403088\n","Total Timesteps: 487310 Episode Num: 503 Reward: 2588.994976363234\n","Total Timesteps: 488310 Episode Num: 504 Reward: 2642.9360893616035\n","Total Timesteps: 489310 Episode Num: 505 Reward: 2341.5535699078964\n","Total Timesteps: 490310 Episode Num: 506 Reward: 2556.886264964492\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2609.145409\n","---------------------------------------\n","Total Timesteps: 491310 Episode Num: 507 Reward: 2565.2441613887413\n","Total Timesteps: 492310 Episode Num: 508 Reward: 2622.056716946973\n","Total Timesteps: 493310 Episode Num: 509 Reward: 2550.869773133566\n","Total Timesteps: 494310 Episode Num: 510 Reward: 2579.7954356639784\n","Total Timesteps: 495310 Episode Num: 511 Reward: 2598.2913716257476\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2604.466089\n","---------------------------------------\n","Total Timesteps: 496310 Episode Num: 512 Reward: 2508.9050029655277\n","Total Timesteps: 497310 Episode Num: 513 Reward: 2625.8810485736976\n","Total Timesteps: 498310 Episode Num: 514 Reward: 2662.937768465988\n","Total Timesteps: 499310 Episode Num: 515 Reward: 2588.4544239119796\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2701.100806\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"wi6e2-_pu05e"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"oW4d1YAMqif1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671232211466,"user_tz":-540,"elapsed":82190,"user":{"displayName":"최세훈","userId":"00148754616075165078"}},"outputId":"7f2a0ddd-edd2-4b7d-f4d6-847cca1d333a"},"source":["class Actor(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(state_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x)) \n","    return x\n","\n","class Critic(nn.Module):\n","  \n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    # Defining the first Critic neural network\n","    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","    # Defining the second Critic neural network\n","    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    # Forward-Propagation on the first Critic Neural Network\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    # Forward-Propagation on the second Critic Neural Network\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","    return x1, x2\n","\n","  def Q1(self, x, u):\n","    xu = torch.cat([x, u], 1)\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","    return x1\n","\n","# Selecting the device (CPU or GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Building the whole Training Process into a class\n","\n","class TD3(object):\n","  \n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n","    \n","    for it in range(iterations):\n","      \n","      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","      \n","      # Step 5: From the next state s’, the Actor target plays the next action a’\n","      next_action = self.actor_target(next_state)\n","      \n","      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clip, noise_clip)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","      \n","      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","      \n","      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n","      target_Q = torch.min(target_Q1, target_Q2)\n","      \n","      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","      \n","      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n","      current_Q1, current_Q2 = self.critic(state, action)\n","      \n","      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","      \n","      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","      \n","      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n","      if it % policy_freq == 0:\n","        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        \n","        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","        \n","        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","  \n","  # Making a save method to save a trained model\n","  def save(self, filename, directory):\n","    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n","    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n","  \n","  # Making a load method to load a pre-trained model\n","  def load(self, filename, directory):\n","    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n","    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n","\n","def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print (\"---------------------------------------\")\n","  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n","  print (\"---------------------------------------\")\n","  return avg_reward\n","\n","env_name = \"AntBulletEnv-v0\"\n","seed = 0\n","\n","file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print (\"---------------------------------------\")\n","print (\"Settings: %s\" % (file_name))\n","print (\"---------------------------------------\")\n","\n","eval_episodes = 10\n","save_env_vid = True\n","env = gym.make(env_name)\n","max_episode_steps = env._max_episode_steps\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()\n","env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])\n","policy = TD3(state_dim, action_dim, max_action)\n","policy.load(file_name, './pytorch_models/')\n","_ = evaluate_policy(policy, eval_episodes=eval_episodes)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------------\n","Settings: TD3_AntBulletEnv-v0_0\n","---------------------------------------\n","---------------------------------------\n","Average Reward over the Evaluation Step: 2695.361549\n","---------------------------------------\n"]}]}]}